{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430eb14d-8731-48a6-a319-29eee0742d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def get_credentials():\n",
    "\treturn {\n",
    "\t\t\"url\" : \"https://eu-de.ml.cloud.ibm.com\",\n",
    "\t\t\"apikey\" :\"18AV1meKNbDmaIRYiqL2iG8XPLPovn8nCk5exIVOm5G5\"\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4cb6ef-1a5f-4f11-9d0a-f47d1f087c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"max_new_tokens\": 4096,\n",
    "    \"repetition_penalty\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4908757-dfb8-4bf8-93fd-d3207a254c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ibm_watsonx_ai in c:\\users\\roro1\\anaconda3\\lib\\site-packages (1.1.22)\n",
      "Requirement already satisfied: requests in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (2.32.2)\n",
      "Requirement already satisfied: httpx in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (0.27.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (2.2.2)\n",
      "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (2.1.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (2024.6.2)\n",
      "Requirement already satisfied: lomond in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (0.3.3)\n",
      "Requirement already satisfied: tabulate in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (0.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (23.2)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (2.13.6)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm_watsonx_ai) (7.0.1)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.13.6 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watsonx_ai) (2.13.6)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.6 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watsonx_ai) (2.13.6)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watsonx_ai) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watsonx_ai) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm_watsonx_ai) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm_watsonx_ai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm_watsonx_ai) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests->ibm_watsonx_ai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests->ibm_watsonx_ai) (3.7)\n",
      "Requirement already satisfied: anyio in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from httpx->ibm_watsonx_ai) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from httpx->ibm_watsonx_ai) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from httpx->ibm_watsonx_ai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->ibm_watsonx_ai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from importlib-metadata->ibm_watsonx_ai) (3.17.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from lomond->ibm_watsonx_ai) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm_watsonx_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e082df35-f2c3-4229-8572-d8975fdc5ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement project_lib (from versions: none)\n",
      "ERROR: No matching distribution found for project_lib\n"
     ]
    }
   ],
   "source": [
    "!pip install project_lib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02297329-2b0e-4088-89f6-8ae29f5bfc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sdaia/allam-1-13b-instruct\"\n",
    "project_id = \"7c5c2625-4a94-4761-8385-ada9c69cf56d\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0e8f58-1ac1-423f-be1e-91d160057314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roro1\\anaconda3\\Lib\\site-packages\\ibm_watsonx_ai\\foundation_models\\model.py:101: DeprecationWarning: The `Model` class is deprecated and will be removed in a future release. Please use the `ModelInference` class instead. To update your imports, use: `from ibm_watsonx_ai.foundation_models import ModelInference`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "\n",
    "model = Model(\n",
    "\tmodel_id = model_id,\n",
    "\tparams = parameters,\n",
    "\tcredentials = get_credentials(),\n",
    "\tproject_id = project_id,\n",
    "\t#space_id = space_id\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4eb9b3-9538-462d-b795-ec89513bca3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'project_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproject_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Project\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'project_lib'"
     ]
    }
   ],
   "source": [
    "from project_lib import Project\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from docx import Document\n",
    "import json\n",
    "\n",
    "# Set up the project with your project ID and access token\n",
    "project = Project(project_id=\"7c5c2625-4a94-4761-8385-ada9c69cf56d\", project_access_token=\"p-2+TGQUwXCynlnJDVx49g+kZA==;VKKGK8NN6i127ReRzoShYQ==:+UgvoPXOjF8hQ4lZUhn+PftCNcQLSnLDqNEbCl19APPOLRmtkqcGO5vkfp3xmQFP9HYtl+CmW4aSRVM7on1nn/BwGDJhVRCj7Q==\")\n",
    "\n",
    "# Function to read text from a Word document file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs if para.text.strip()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1939696d-8591-43e1-a2b6-86a56b3c2406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\roro1\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2d2544-d8a1-4546-8452-70fec5565ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 404: <!DOCTYPE html>\n",
      "<html class=\"h-full\" lang=\"en-US\" dir=\"ltr\">\n",
      "  <head>\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Regular-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-RegularItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Medium-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Semibold-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-MediumItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-Text.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-TextItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBold.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBoldItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta name=\"author\" content=\"ngrok\">\n",
      "    <meta name=\"description\" content=\"ngrok is the fastest way to put anything on the internet with a single command.\">\n",
      "    <meta name=\"robots\" content=\"noindex, nofollow\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "    <link id=\"style\" rel=\"stylesheet\" href=\"https://cdn.ngrok.com/static/css/error.css\">\n",
      "    <noscript>Tunnel a904-85-194-96-74.ngrok-free.app not found (ERR_NGROK_3200)</noscript>\n",
      "    <script id=\"script\" src=\"https://cdn.ngrok.com/static/js/error.js\" type=\"text/javascript\"></script>\n",
      "  </head>\n",
      "  <body class=\"h-full\" id=\"ngrok\">\n",
      "    <div id=\"root\" data-payload=\"eyJjZG5CYXNlIjoiaHR0cHM6Ly9jZG4ubmdyb2suY29tLyIsImNvZGUiOiIzMjAwIiwibWVzc2FnZSI6IlR1bm5lbCBhOTA0LTg1LTE5NC05Ni03NC5uZ3Jvay1mcmVlLmFwcCBub3QgZm91bmQiLCJ0aXRsZSI6Ik5vdCBGb3VuZCJ9\"></div>\n",
      "  </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://a904-85-194-96-74.ngrok-free.app/generate_grammar_explanation'\n",
    "\n",
    "# Create a dictionary with the data you want to send\n",
    "data = {\n",
    "    'text': 'Your sentence or query for grammar explanation'\n",
    "}\n",
    "\n",
    "# Send a POST request\n",
    "response = requests.post(url, data=data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    # Process the response, assuming it's a JSON\n",
    "    explanation = response.json()\n",
    "    print(\"Grammar Explanation:\", explanation)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16263d66-625f-40b6-b71b-8c0b9424d5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar Explanation: {'answer': ' حرف المد هو حرف يُضاف إلى الكلمة العربية لإطالة الصوت وتسهيل النطق. يوجد في اللغة العربية ثلاثة حروف مد وهي:\\n\\n1. الألف (ا): تُضاف إلى الكلمة لإطالة الصوت وتسهيل النطق، مثل: قال، كتاب، سماء.\\n2. الواو (و): تُضاف إلى الكلمة لإطالة الصوت وتسهيل النطق، مثل: يقول، مولود، كتابٌ.\\n3. الياء (ي): تُضاف إلى الكلمة لإطالة الصوت وتسهيل النطق، مثل: يبيع، بيت، كتابٌ.\\n\\nتُستخدم حروف المد في الشعر والنثر العربي لإضفاء جمالية على النص وتسهيل قراءته وتلاوته. '}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL\n",
    "url = 'http://a904-85-194-96-74.ngrok-free.app/generate_grammar_explanation'\n",
    "\n",
    "# Create a dictionary with the data you want to send\n",
    "data = {\n",
    "    'question': 'ماهو حرف المد'\n",
    "}\n",
    "\n",
    "# Send a POST request with JSON data\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    # Process the response, assuming it's a JSON\n",
    "    explanation = response.json()\n",
    "    print(\"Grammar Explanation:\", explanation)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4305f30a-01b3-4b24-b8c7-f50522569b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from openai==0.28) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from openai==0.28) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from openai==0.28) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\roro1\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai==0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f05cdbf6-696e-42af-8eba-5cf347c645b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing response: 1. Correctness: 4.5 - The model's answer is mostly correct. It does not specify the time of day for the outing, unlike the ground truth answer, but it does correctly identify the season, destination, and company.\n",
      "2. Relevancy: 5 - The model's answer is highly relevant to the question. It provides detailed descriptions of what was seen, heard, smelled, felt, contemplated, and wished for during the outing.\n",
      "3. Fluency: 5 - The answer is very fluent. It reads naturally and is grammatically correct.\n",
      "4. Completeness: 5 - The model's answer is complete. It addresses all aspects of the question, providing detailed and thoughtful responses to each part. - invalid literal for int() with base 10: '4.5'\n",
      "Error parsing response: 1. Correctness: 5/5. The model's answer matches the ground truth perfectly.\n",
      "2. Relevancy: 5/5. The model's answer is directly relevant to the question.\n",
      "3. Fluency: 5/5. The answer is written in fluent, grammatically correct Arabic.\n",
      "4. Completeness: 5/5. The model's answer fully addresses the question. - invalid literal for int() with base 10: '5/5.'\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV data\n",
    "df = pd.read_csv(\"Accuracy - Sheet1.csv\")  # Replace with your file path\n",
    "\n",
    "# Define API key for OpenAI\n",
    "openai.api_key = \"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\"\n",
    "def evaluate_answer(question, model_answer, ground_truth):\n",
    "    # Prepare the messages for the chat API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert evaluator of AI-generated answers.\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"Evaluate the following answer on a scale of 1 to 5 for correctness, relevancy, fluency, and completeness.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Ground Truth Answer: {ground_truth}\\n\\n\"\n",
    "            f\"Model Answer: {model_answer}\\n\\n\"\n",
    "            f\"Provide a score from 1 to 5 for each metric:\\n\"\n",
    "            f\"1. Correctness: How correct is the answer compared to the ground truth?\\n\"\n",
    "            f\"2. Relevancy: How relevant is the answer to the question?\\n\"\n",
    "            f\"3. Fluency: Does the answer read naturally and grammatically correct?\\n\"\n",
    "            f\"4. Completeness: Does it address all aspects of the question?\\n\"\n",
    "        )}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",  # Use \"gpt-4\" or \"gpt-3.5-turbo\"\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            temperature=0  # For deterministic output\n",
    "        )\n",
    "        \n",
    "        # Extract scores and explanation from the response\n",
    "        result_text = response.choices[0].message['content'].strip()\n",
    "        \n",
    "        # Attempt to parse each metric\n",
    "        correctness = relevancy = fluency = completeness = None\n",
    "        for line in result_text.splitlines():\n",
    "            if \"Correctness:\" in line:\n",
    "                correctness = int(line.split(\"Correctness:\")[1].strip().split()[0])\n",
    "            elif \"Relevancy:\" in line:\n",
    "                relevancy = int(line.split(\"Relevancy:\")[1].strip().split()[0])\n",
    "            elif \"Fluency:\" in line:\n",
    "                fluency = int(line.split(\"Fluency:\")[1].strip().split()[0])\n",
    "            elif \"Completeness:\" in line:\n",
    "                completeness = int(line.split(\"Completeness:\")[1].strip().split()[0])\n",
    "        \n",
    "    except openai.error.RateLimitError:\n",
    "        print(\"Rate limit exceeded. Waiting to retry...\")\n",
    "        time.sleep(10)  # Wait for a few seconds before retrying\n",
    "        return evaluate_answer(question, model_answer, ground_truth)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {result_text} - {e}\")\n",
    "        return None, None, None, None, result_text\n",
    "\n",
    "    return correctness, relevancy, fluency, completeness, result_text\n",
    "\n",
    "# Apply evaluation for each row\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    question = row[\"Question\"]\n",
    "    ground_truth = row[\"Truth_Output\"]\n",
    "    \n",
    "    # RAG model evaluation\n",
    "    rag_answer = row[\"LLM_Output\"]\n",
    "    rag_correctness, rag_relevancy, rag_fluency, rag_completeness, rag_eval = evaluate_answer(question, rag_answer, ground_truth)\n",
    "    \n",
    "    # Regular model evaluation\n",
    "    regular_answer = row[\"Base Allam\"]\n",
    "    reg_correctness, reg_relevancy, reg_fluency, reg_completeness, reg_eval = evaluate_answer(question, regular_answer, ground_truth)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"rag_answer\": rag_answer,\n",
    "        \"rag_correctness\": rag_correctness,\n",
    "        \"rag_relevancy\": rag_relevancy,\n",
    "        \"rag_fluency\": rag_fluency,\n",
    "        \"rag_completeness\": rag_completeness,\n",
    "        \"regular_answer\": regular_answer,\n",
    "        \"reg_correctness\": reg_correctness,\n",
    "        \"reg_relevancy\": reg_relevancy,\n",
    "        \"reg_fluency\": reg_fluency,\n",
    "        \"reg_completeness\": reg_completeness,\n",
    "        \"rag_eval\": rag_eval,\n",
    "        \"reg_eval\": reg_eval,\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"evaluation_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75f5dfce-10af-46b9-a31c-4e7371c9021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric  RAG Model  Regular Model\n",
      "0   Correctness   3.857143         3.2500\n",
      "1     Relevancy   4.571429         4.1875\n",
      "2       Fluency   5.000000         5.0000\n",
      "3  Completeness   4.714286         3.8750\n"
     ]
    }
   ],
   "source": [
    "# Load the evaluation results\n",
    "results_df = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "# Calculate the average scores for each metric for both RAG and Regular models\n",
    "average_scores = {\n",
    "    \"Metric\": [\"Correctness\", \"Relevancy\", \"Fluency\", \"Completeness\"],\n",
    "    \"RAG Model\": [\n",
    "        results_df[\"rag_correctness\"].mean(),\n",
    "        results_df[\"rag_relevancy\"].mean(),\n",
    "        results_df[\"rag_fluency\"].mean(),\n",
    "        results_df[\"rag_completeness\"].mean()\n",
    "    ],\n",
    "    \"Regular Model\": [\n",
    "        results_df[\"reg_correctness\"].mean(),\n",
    "        results_df[\"reg_relevancy\"].mean(),\n",
    "        results_df[\"reg_fluency\"].mean(),\n",
    "        results_df[\"reg_completeness\"].mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the average scores dictionary into a DataFrame for easy viewing\n",
    "average_scores_df = pd.DataFrame(average_scores)\n",
    "\n",
    "# Save the average scores to a CSV file\n",
    "average_scores_df.to_csv(\"average_scores_comparison.csv\", index=False)\n",
    "\n",
    "# Display the average scores\n",
    "print(average_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b53a747-3f8f-47c8-9a03-a4362a044c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing response: 1. Correctness: 4.5 - The model answer is mostly correct. It aligns well with the ground truth answer, but it does not specify the time of the day when the outing took place.\n",
      "2. Relevancy: 5 - The model answer is highly relevant to the question. It provides detailed descriptions of the experience in the garden, which is exactly what the question asked for.\n",
      "3. Fluency: 5 - The answer is very fluent. It reads naturally and is grammatically correct.\n",
      "4. Completeness: 5 - The model answer is complete. It addresses all aspects of the question, providing detailed descriptions of what was seen, heard, smelled, felt, contemplated, and wished for during the outing in - invalid literal for int() with base 10: '4.5'\n",
      "Error parsing response: 1. Correctness: 1/5 - The model answer is not correct. It includes a lot of irrelevant information and does not match the ground truth answer.\n",
      "2. Relevancy: 1/5 - The model answer is not relevant to the question. The question asks to mimic a sentence, but the model answer provides a lengthy and unrelated response.\n",
      "3. Fluency: 5/5 - Despite being incorrect and irrelevant, the model answer is fluent and grammatically correct in Arabic.\n",
      "4. Completeness: 1/5 - The model answer does not address the question at all. It provides a lot of unrelated information instead of mimicking the given sentence. - invalid literal for int() with base 10: '1/5'\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV data\n",
    "df = pd.read_excel(\"Accuracy (3).xlsx\")  # Replace with your file path\n",
    "\n",
    "# Define API key for OpenAI\n",
    "openai.api_key = \"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\"\n",
    "def evaluate_answer(question, model_answer, ground_truth):\n",
    "    # Prepare the messages for the chat API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert evaluator of AI-generated answers.\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"Evaluate the following answer on a scale of 1 to 5 for correctness, relevancy, fluency, and completeness.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Ground Truth Answer: {ground_truth}\\n\\n\"\n",
    "            f\"Model Answer: {model_answer}\\n\\n\"\n",
    "            f\"Provide a score from 1 to 5 for each metric:\\n\"\n",
    "            f\"1. Correctness: How correct is the answer compared to the ground truth?\\n\"\n",
    "            f\"2. Relevancy: How relevant is the answer to the question?\\n\"\n",
    "            f\"3. Fluency: Does the answer read naturally and grammatically correct?\\n\"\n",
    "            f\"4. Completeness: Does it address all aspects of the question?\\n\"\n",
    "        )}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",  # Use \"gpt-4\" or \"gpt-3.5-turbo\"\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            temperature=0  # For deterministic output\n",
    "        )\n",
    "        \n",
    "        # Extract scores and explanation from the response\n",
    "        result_text = response.choices[0].message['content'].strip()\n",
    "        \n",
    "        # Attempt to parse each metric\n",
    "        correctness = relevancy = fluency = completeness = None\n",
    "        for line in result_text.splitlines():\n",
    "            if \"Correctness:\" in line:\n",
    "                correctness = int(line.split(\"Correctness:\")[1].strip().split()[0])\n",
    "            elif \"Relevancy:\" in line:\n",
    "                relevancy = int(line.split(\"Relevancy:\")[1].strip().split()[0])\n",
    "            elif \"Fluency:\" in line:\n",
    "                fluency = int(line.split(\"Fluency:\")[1].strip().split()[0])\n",
    "            elif \"Completeness:\" in line:\n",
    "                completeness = int(line.split(\"Completeness:\")[1].strip().split()[0])\n",
    "        \n",
    "    except openai.error.RateLimitError:\n",
    "        print(\"Rate limit exceeded. Waiting to retry...\")\n",
    "        time.sleep(10)  # Wait for a few seconds before retrying\n",
    "        return evaluate_answer(question, model_answer, ground_truth)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {result_text} - {e}\")\n",
    "        return None, None, None, None, result_text\n",
    "\n",
    "    return correctness, relevancy, fluency, completeness, result_text\n",
    "\n",
    "# Apply evaluation for each row\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    question = row[\"Question\"]\n",
    "    ground_truth = row[\"Truth_Output\"]\n",
    "    \n",
    "    # RAG model evaluation\n",
    "    rag_answer = row[\"LLM_Output\"]\n",
    "    rag_correctness, rag_relevancy, rag_fluency, rag_completeness, rag_eval = evaluate_answer(question, rag_answer, ground_truth)\n",
    "    \n",
    "    # Regular model evaluation\n",
    "    regular_answer = row[\"Base Allam\"]\n",
    "    reg_correctness, reg_relevancy, reg_fluency, reg_completeness, reg_eval = evaluate_answer(question, regular_answer, ground_truth)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"rag_answer\": rag_answer,\n",
    "        \"rag_correctness\": rag_correctness,\n",
    "        \"rag_relevancy\": rag_relevancy,\n",
    "        \"rag_fluency\": rag_fluency,\n",
    "        \"rag_completeness\": rag_completeness,\n",
    "        \"regular_answer\": regular_answer,\n",
    "        \"reg_correctness\": reg_correctness,\n",
    "        \"reg_relevancy\": reg_relevancy,\n",
    "        \"reg_fluency\": reg_fluency,\n",
    "        \"reg_completeness\": reg_completeness,\n",
    "        \"rag_eval\": rag_eval,\n",
    "        \"reg_eval\": reg_eval,\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"evaluation_results2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3eb8a1e-dccf-4c0d-bd83-bc5ea6d2a0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric  RAG Model  Regular Model\n",
      "0   Correctness   4.142857         3.1875\n",
      "1     Relevancy   4.571429         4.0625\n",
      "2       Fluency   5.000000         5.0000\n",
      "3  Completeness   4.714286         3.7500\n"
     ]
    }
   ],
   "source": [
    "# Load the evaluation results\n",
    "results_df = pd.read_csv(\"evaluation_results2.csv\")\n",
    "\n",
    "# Calculate the average scores for each metric for both RAG and Regular models\n",
    "average_scores = {\n",
    "    \"Metric\": [\"Correctness\", \"Relevancy\", \"Fluency\", \"Completeness\"],\n",
    "    \"RAG Model\": [\n",
    "        results_df[\"rag_correctness\"].mean(),\n",
    "        results_df[\"rag_relevancy\"].mean(),\n",
    "        results_df[\"rag_fluency\"].mean(),\n",
    "        results_df[\"rag_completeness\"].mean()\n",
    "    ],\n",
    "    \"Regular Model\": [\n",
    "        results_df[\"reg_correctness\"].mean(),\n",
    "        results_df[\"reg_relevancy\"].mean(),\n",
    "        results_df[\"reg_fluency\"].mean(),\n",
    "        results_df[\"reg_completeness\"].mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the average scores dictionary into a DataFrame for easy viewing\n",
    "average_scores_df = pd.DataFrame(average_scores)\n",
    "\n",
    "# Save the average scores to a CSV file\n",
    "average_scores_df.to_csv(\"average_scores_comparison2.csv\", index=False)\n",
    "\n",
    "# Display the average scores\n",
    "print(average_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e6d4b12-aa87-474f-88d8-f288b2f1cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Metric  RAG Model  Regular Model\n",
      "0                Simplicity     4.2500         4.0625\n",
      "1                Engagement     3.3750         3.5000\n",
      "2  Age-Appropriate Language     4.3125         4.0625\n",
      "3                  Accuracy     4.5625         3.7500\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load CSV data\n",
    "df = pd.read_excel(\"Accuracy (3).xlsx\")  # Replace with your file path\n",
    "\n",
    "# Define API key for OpenAI\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def evaluate_answer_for_kids(question, model_answer, ground_truth):\n",
    "    # Prepare the messages for the chat API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at evaluating AI-generated answers for children.\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"Evaluate the following answer based on its simplicity, engagement, age-appropriate language, and accuracy. \"\n",
    "            f\"Rate each metric on a scale from 1 to 5.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Ground Truth Answer: {ground_truth}\\n\\n\"\n",
    "            f\"Model Answer: {model_answer}\\n\\n\"\n",
    "            f\"Provide a score from 1 to 5 for each metric:\\n\"\n",
    "            f\"1. Simplicity: Is the answer simple and easy for kids to understand?\\n\"\n",
    "            f\"2. Engagement: How engaging or interesting is the answer for kids?\\n\"\n",
    "            f\"3. Age-Appropriate Language: Does the answer use language suitable for kids?\\n\"\n",
    "            f\"4. Accuracy: Is the information correct and understandable for kids?\\n\"\n",
    "        )}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            temperature=0  # For consistent output\n",
    "        )\n",
    "        \n",
    "        # Extract scores from the response\n",
    "        result_text = response.choices[0].message['content'].strip()\n",
    "        \n",
    "        # Attempt to parse each metric\n",
    "        simplicity = engagement = age_appropriateness = accuracy = None\n",
    "        for line in result_text.splitlines():\n",
    "            if \"Simplicity:\" in line:\n",
    "                simplicity = int(line.split(\"Simplicity:\")[1].strip().split()[0])\n",
    "            elif \"Engagement:\" in line:\n",
    "                engagement = int(line.split(\"Engagement:\")[1].strip().split()[0])\n",
    "            elif \"Age-Appropriate Language:\" in line:\n",
    "                age_appropriateness = int(line.split(\"Age-Appropriate Language:\")[1].strip().split()[0])\n",
    "            elif \"Accuracy:\" in line:\n",
    "                accuracy = int(line.split(\"Accuracy:\")[1].strip().split()[0])\n",
    "        \n",
    "    except openai.error.RateLimitError:\n",
    "        print(\"Rate limit exceeded. Waiting to retry...\")\n",
    "        time.sleep(10)  # Wait before retrying\n",
    "        return evaluate_answer_for_kids(question, model_answer, ground_truth)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {result_text} - {e}\")\n",
    "        return None, None, None, None, result_text\n",
    "\n",
    "    return simplicity, engagement, age_appropriateness, accuracy, result_text\n",
    "\n",
    "# Apply evaluation for each row\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    question = row[\"Question\"]\n",
    "    ground_truth = row[\"Truth_Output\"]\n",
    "    \n",
    "    # RAG model evaluation for kids\n",
    "    rag_answer = row[\"LLM_Output\"]\n",
    "    rag_simplicity, rag_engagement, rag_age_appropriateness, rag_accuracy, rag_eval = evaluate_answer_for_kids(question, rag_answer, ground_truth)\n",
    "    \n",
    "    # Regular model evaluation for kids\n",
    "    regular_answer = row[\"Base Allam\"]\n",
    "    reg_simplicity, reg_engagement, reg_age_appropriateness, reg_accuracy, reg_eval = evaluate_answer_for_kids(question, regular_answer, ground_truth)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"rag_answer\": rag_answer,\n",
    "        \"rag_simplicity\": rag_simplicity,\n",
    "        \"rag_engagement\": rag_engagement,\n",
    "        \"rag_age_appropriateness\": rag_age_appropriateness,\n",
    "        \"rag_accuracy\": rag_accuracy,\n",
    "        \"regular_answer\": regular_answer,\n",
    "        \"reg_simplicity\": reg_simplicity,\n",
    "        \"reg_engagement\": reg_engagement,\n",
    "        \"reg_age_appropriateness\": reg_age_appropriateness,\n",
    "        \"reg_accuracy\": reg_accuracy,\n",
    "        \"rag_eval\": rag_eval,\n",
    "        \"reg_eval\": reg_eval,\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"evaluation_results_for_kids2.csv\", index=False)\n",
    "\n",
    "# Displaying average scores for both models\n",
    "average_scores = {\n",
    "    \"Metric\": [\"Simplicity\", \"Engagement\", \"Age-Appropriate Language\", \"Accuracy\"],\n",
    "    \"RAG Model\": [\n",
    "        results_df[\"rag_simplicity\"].mean(),\n",
    "        results_df[\"rag_engagement\"].mean(),\n",
    "        results_df[\"rag_age_appropriateness\"].mean(),\n",
    "        results_df[\"rag_accuracy\"].mean()\n",
    "    ],\n",
    "    \"Regular Model\": [\n",
    "        results_df[\"reg_simplicity\"].mean(),\n",
    "        results_df[\"reg_engagement\"].mean(),\n",
    "        results_df[\"reg_age_appropriateness\"].mean(),\n",
    "        results_df[\"reg_accuracy\"].mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the average scores dictionary into a DataFrame and save it\n",
    "average_scores_df = pd.DataFrame(average_scores)\n",
    "average_scores_df.to_csv(\"average_scores_for_kids_comparison2.csv\", index=False)\n",
    "\n",
    "# Display the average scores\n",
    "print(average_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd1742-07be-44e3-9637-6eeaa7728b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
